{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee35d3a-f500-4bff-9b74-deb54a44a966",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'netCDF4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Author: Kyeong Pil Kong (USC)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This code converts chl data downloaded from the Copernicus GlobColour database into simplified csv files. \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The csv files are used to match them onto locations from parcel back trajectories\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetCDF4\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'netCDF4'"
     ]
    }
   ],
   "source": [
    "# Author: Kyeong Pil Kong (USC)\n",
    "# kyeongpi@usc.edu\n",
    "# This code converts chl data downloaded from the Copernicus GlobColour database into simplified csv files. \n",
    "# https://data.marine.copernicus.eu/product/OCEANCOLOUR_GLO_BGC_L4_MY_009_104/files?subdataset=cmems_obs-oc_glo_bgc-plankton_my_l4-gapfree-multi-4km_P1D_202311\n",
    "# The csv files are used to match them onto locations from parcel back trajectories\n",
    "\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the geographical extent\n",
    "extent_to_keep = [-180, -120, -10, 40]\n",
    "\n",
    "# Specify the directory containing the .nc files\n",
    "nc_files_directory = #your filepath here\n",
    "\n",
    "# Get a list of all .nc files in the directory\n",
    "nc_files = [file for file in os.listdir(nc_files_directory) if file.endswith('.nc')]\n",
    "\n",
    "# Iterate through each .nc file\n",
    "for nc_file_name in nc_files:\n",
    "    # Extract date from the filename and remove the .nc extension\n",
    "    date_str = nc_file_name.split('_')[0].split('.')[0]\n",
    "\n",
    "    \n",
    "    # Convert date_str to a datetime object\n",
    "    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "    \n",
    "    # Format the date string as YYYY_MM_DD\n",
    "    formatted_date_str = date_obj.strftime('%Y_%m_%d')\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    nc_file_path = os.path.join(nc_files_directory, nc_file_name)\n",
    "    nc_file = netCDF4.Dataset(nc_file_path, 'r')\n",
    "\n",
    "    # Extract latitude, longitude, and chlor_a data\n",
    "    latitude = nc_file.variables['lat'][:]\n",
    "    longitude = nc_file.variables['lon'][:]\n",
    "    chlor_a_data = nc_file.variables['CHL'][0, :, :]\n",
    "\n",
    "    # Create a meshgrid of latitude and longitude\n",
    "    lon_grid, lat_grid = np.meshgrid(longitude, latitude)\n",
    "\n",
    "    # Create a boolean mask based on the extent\n",
    "    mask = (lon_grid >= extent_to_keep[0]) & (lon_grid <= extent_to_keep[1]) & (lat_grid >= extent_to_keep[2]) & (lat_grid <= extent_to_keep[3])\n",
    "\n",
    "    # Apply the mask to chlor_a_data\n",
    "    chlor_a_data_filtered = np.ma.masked_where(~mask, chlor_a_data)\n",
    "\n",
    "    # Close the NetCDF file\n",
    "    nc_file.close()\n",
    "    \n",
    "    # Convert the filtered data to a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Latitude': lat_grid[mask].flatten(),\n",
    "        'Longitude': lon_grid[mask].flatten(),\n",
    "        'Chlorophyll-a': chlor_a_data_filtered[mask].flatten()  # Use chlor_a_data_filtered instead of chl_data\n",
    "    })\n",
    "    \n",
    "    # Remove rows where 'Chlorophyll-a' is empty\n",
    "    df = df.dropna(subset=['Chlorophyll-a'])\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    csv_file_name = f'chl_sorted_{formatted_date_str}.csv'\n",
    "    csv_file_path = os.path.join(nc_files_directory, csv_file_name)\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    # print(f'Saved {csv_file_name}')\n",
    "\n",
    "print('Processing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06528ee-5463-4584-8f7e-87fa0e6e9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert mimi netcdf file outputs to a dataframe\n",
    "# Data source: https://doi.org/10.5194/gmd-12-3835-2019\n",
    "# Data Author: Douglas Hamilton, NCSU\n",
    "\n",
    "import pandas as pd\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "\n",
    "# Open the NetCDF file\n",
    "nc_file = netCDF4.Dataset(filepath, 'r+')\n",
    "\n",
    "# Extract relevant variables\n",
    "dates = nc_file.variables['date'][:]  # Assuming 'date' is in a suitable format\n",
    "latitude = nc_file.variables['lat'][:]\n",
    "longitude = nc_file.variables['lon'][:]\n",
    "fesolsrf_data = nc_file.variables['FEsolSRF'][:,:,:]\n",
    "fesolsrf_data = nc_file.variables['FESOLSRF'][:,:,:]\n",
    "feallsrf_data = fesolsrf_data + fesolsrf_data\n",
    "# dusoldry_data = nc_file.variables['DUsolDRY'][:,:,:]\n",
    "# dusolwet_data = nc_file.variables['DUsolWET'][:,:,:]\n",
    "# dusolall_data = dusoldry_data + dusolwet_data\n",
    "feansolsrf_data = nc_file.variables['FEANsolSRF'][:,:,:]\n",
    "feansolsrf_data = nc_file.variables['FEANSOLSRF'][:,:,:]\n",
    "feanallsrf_data = feansolsrf_data + feansolsrf_data\n",
    "febbsolsrf_data = nc_file.variables['FEBBsolSRF'][:,:,:]\n",
    "febbsolsrf_data = nc_file.variables['FEBBSOLSRF'][:,:,:]\n",
    "febballsrf_data = febbsolsrf_data + febbsolsrf_data\n",
    "fedusolsrf_data = nc_file.variables['FEDUsolSRF'][:,:,:]\n",
    "fedusolsrf_data = nc_file.variables['FEDUSOLSRF'][:,:,:]\n",
    "feduallsrf_data = fedusolsrf_data + fedusolsrf_data\n",
    "\n",
    "# Reshape the data for tabulation\n",
    "dates_grid, latitude_grid, longitude_grid = np.meshgrid(dates, latitude, longitude, indexing='ij')\n",
    "dates_flat = dates_grid.flatten()\n",
    "latitude_flat = latitude_grid.flatten()\n",
    "longitude_flat = longitude_grid.flatten()\n",
    "fesolsrf_flat = fesolsrf_data.flatten()\n",
    "fesolsrf_flat = fesolsrf_data.flatten()\n",
    "feansolsrf_flat = feansolsrf_data.flatten()\n",
    "feansolsrf_flat = feansolsrf_data.flatten()\n",
    "febbsolsrf_flat = febbsolsrf_data.flatten()\n",
    "febbsolsrf_flat = febbsolsrf_data.flatten()\n",
    "fedusolsrf_flat = fedusolsrf_data.flatten()\n",
    "fedusolsrf_flat = fedusolsrf_data.flatten()\n",
    "\n",
    "# Create a DataFrame\n",
    "df_mimi = pd.DataFrame({\n",
    "    'Date': dates_flat,\n",
    "    'Latitude': latitude_flat,\n",
    "    'Longitude': longitude_flat,\n",
    "    'FEsolSRF': fesolsrf_flat,\n",
    "    'FESOLSRF': fesolsrf_flat,\n",
    "    'FEANsolSRF': feansolsrf_flat,\n",
    "    'FEBBsolSRF': febbsolsrf_flat,\n",
    "    'FEDUTOSRF': fedusolsrf_flat,\n",
    "    'FEANSOLSRF': feansolsrf_flat,\n",
    "    'FEBBSOLSRF': febbsolsrf_flat,\n",
    "    'FEDUSOLSRF': fedusolsrf_flat,\n",
    "})\n",
    "\n",
    "# Close the NetCDF file\n",
    "nc_file.close()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_mimi)\n",
    "\n",
    "#convert mimi output as individual csv's per day\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Sort the DataFrame by Date\n",
    "df_sorted = df_mimi.sort_values(by='Date')\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Convert the 'Date' column to datetime format for better handling\n",
    "df_mimi['Date'] = pd.to_datetime(df_mimi['Date'], format='%Y%m%d')\n",
    "\n",
    "# Convert longitude values to the standard range of -180 to 180\n",
    "df_mimi['Longitude'] = (df_mimi['Longitude'] + 180) % 360 - 180\n",
    "\n",
    "# Sort the DataFrame by Date\n",
    "df_sorted = df_mimi.sort_values(by='Date')\n",
    "\n",
    "# Group by Date and iterate over groups to save each group to a separate CSV file\n",
    "for date, group in df_sorted.groupby('Date'):\n",
    "    # Generate the file name with the format 'mimi_YYYY-MM-DD.csv'\n",
    "    output_filepath = f\"FILEPATH/mimi_srf_{date.strftime('%Y_%m_%d')}.csv\"\n",
    "    file_name = output_filepath\n",
    "    \n",
    "    # Save the group to the CSV file\n",
    "    group.to_csv(file_name, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
